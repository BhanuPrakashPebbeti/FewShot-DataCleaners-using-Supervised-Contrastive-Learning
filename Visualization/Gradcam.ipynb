{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of additional data : 2780\n",
      "Length of data : 9632\n",
      "Length of labels : 9632\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import albumentations as A\n",
    "import torch\n",
    "torch.__version__\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.multiprocessing import cpu_count\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from argparse import Namespace\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
    "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
    "            \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        \"\"\"\n",
    "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
    "        z_i, z_j as per SimCLR paper\n",
    "        \"\"\"\n",
    "        z_i = F.normalize(emb_i, dim=1)\n",
    "        z_j = F.normalize(emb_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        \n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        denominator = self.negatives_mask * torch.exp(similarity_matrix / self.temperature)\n",
    "    \n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
    "        return loss\n",
    "    \n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "       \n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "     \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "img_dirs = []\n",
    "labels_temp = []\n",
    "additional_data = []\n",
    "bad_type1_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/bad_files1.txt'\n",
    "bad_type2_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/files2bad.txt'\n",
    "bad_type3_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/bad_files3.txt'\n",
    "for class_,txt in enumerate([bad_type1_txt,bad_type2_txt,bad_type3_txt]):\n",
    "    for i,dir_ in enumerate(pickle.load(open(txt,'rb'))):\n",
    "        dir_ = dir_.split('/')\n",
    "        dir_[0] = '/workstation'\n",
    "        dir_[1] = 'home'\n",
    "        dir_.insert(2,'bijoy')\n",
    "        dir_.insert(3,'data_from_b170007ec')\n",
    "        dir_.insert(6,'train')\n",
    "        if 'additional' in dir_[-3].split('_'):\n",
    "            dir_.remove(dir_[-2])\n",
    "        else:\n",
    "            dir_.remove(dir_[-3])\n",
    "        dir_ = '/'.join(dir_)\n",
    "        additional_data.append(dir_)\n",
    "print(\"Length of additional data :\",len(additional_data))\n",
    "\n",
    "type1_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/good_files1.txt'\n",
    "type2_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/files2good.txt'\n",
    "type3_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/good_files3.txt'\n",
    "for class_,txt in enumerate([type1_txt,type2_txt,type3_txt]):\n",
    "    for i,dir_ in enumerate(pickle.load(open(txt,'rb'))):\n",
    "        dir_ = dir_.split('/')\n",
    "        dir_[0] = '/workstation'\n",
    "        dir_[1] = 'home'\n",
    "        dir_.insert(2,'bijoy')\n",
    "        dir_.insert(3,'data_from_b170007ec')\n",
    "        dir_.insert(6,'train')\n",
    "        if 'additional' in dir_[-3].split('_'):\n",
    "            dir_.remove(dir_[-2])\n",
    "        else:\n",
    "            dir_.remove(dir_[-3])\n",
    "        dir_ = '/'.join(dir_)\n",
    "        img_dirs.append(dir_)\n",
    "        labels_temp.append(0)\n",
    "\n",
    "files = os.listdir(\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Imagenet/val\")\n",
    "files = sorted(files)\n",
    "for i in files[1:]:\n",
    "    path = \"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Imagenet/val/\" + str(i)\n",
    "    img = Image.open(path)\n",
    "    t2 = transforms.ToTensor()(img)\n",
    "    if t2.shape[0] == 3:\n",
    "        img_dirs.append(path)\n",
    "        labels_temp.append(1)    \n",
    "data_temp = img_dirs\n",
    "\n",
    "temp = list(zip(data_temp, labels_temp))\n",
    "random.shuffle(temp)\n",
    "data, labels = zip(*temp)\n",
    "# data = data_temp\n",
    "# labels = labels_temp\n",
    "print(\"Length of data :\",len(data))\n",
    "print(\"Length of labels :\",len(labels))\n",
    "\n",
    "def random_rotate(image):\n",
    "    if random.random() > 0.5:\n",
    "        return tvf.rotate(image, angle=random.choice((0, 90, 180, 270)))\n",
    "    return image\n",
    "\n",
    "class ResizedRotation():\n",
    "    def __init__(self, angle, output_size=(512,512)):\n",
    "        self.angle = angle\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def angle_to_rad(self, ang): return np.pi * ang / 180.0\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        new_h = int(np.abs(w * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(h * np.sin(self.angle_to_rad(self.angle))))\n",
    "        new_w = int(np.abs(h * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(w * np.sin(self.angle_to_rad(self.angle))))\n",
    "        img = tvf.resize(image, (new_w, new_h))\n",
    "        img = tvf.rotate(img, self.angle)\n",
    "        img = tvf.center_crop(img, self.output_size)\n",
    "        return img\n",
    "    \n",
    "class WrapWithRandomParams():\n",
    "    def __init__(self, constructor, ranges):\n",
    "        self.constructor = constructor\n",
    "        self.ranges = ranges\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        randoms = [float(np.random.uniform(low, high)) for _, (low, high) in zip(range(len(self.ranges)), self.ranges)]\n",
    "        return self.constructor(*randoms)(image)\n",
    "    \n",
    "class PretrainingDatasetWrapper(Dataset):\n",
    "    def __init__(self, ds: Dataset, l: labels, target_size=(512,512), debug=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.labels = l\n",
    "        self.debug = debug\n",
    "        self.target_size = target_size\n",
    "        if debug:\n",
    "            print(\"DATASET IN DEBUG MODE\")\n",
    "        \n",
    "        # I will be using network pre-trained on ImageNet first, which uses this normalization.\n",
    "        # Remove this, if you're training from scratch or apply different transformations accordingly\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        random_resized_rotation = WrapWithRandomParams(lambda angle: ResizedRotation(angle, target_size), [(0.0, 360.0)])\n",
    "        self.randomize = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(target_size, scale=(340/512, 340/512), ratio=(1.0, 1.0)),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Lambda(random_rotate)\n",
    "            ]),\n",
    "            transforms.RandomApply([\n",
    "                random_resized_rotation\n",
    "            ], p=0.6),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.6, saturation=0.6, hue=0.3)\n",
    "            ], p=0.8)\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        this_image_raw_path = self.ds[idx]\n",
    "        label = self.labels[idx]\n",
    "        this_image_raw = Image.open(this_image_raw_path)\n",
    "        if self.debug:\n",
    "            random.seed(idx)\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            random.seed(idx + 1)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        else:\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        \n",
    "        if preprocess:\n",
    "            t1 = self.preprocess(t1)\n",
    "            t2 = self.preprocess(t2)\n",
    "        else:\n",
    "            t1 = transforms.ToTensor()(t1)\n",
    "            t2 = transforms.ToTensor()(t2)\n",
    "            \n",
    "        return (t1, t2), torch.tensor(label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "    \n",
    "    \n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class ImageEmbedding(nn.Module):       \n",
    "    class Identity(nn.Module):\n",
    "        def __init__(self): super().__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "    \n",
    "        \n",
    "    def __init__(self, embedding_size=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "        internal_embedding_size = base_model._fc.in_features\n",
    "        base_model._fc = ImageEmbedding.Identity()\n",
    "        \n",
    "        self.embedding = base_model\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=internal_embedding_size, out_features=embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embedding_size, out_features=embedding_size)\n",
    "        )\n",
    "\n",
    "    def calculate_embedding(self, image):\n",
    "        return self.embedding(image)\n",
    "\n",
    "    def forward(self, X):\n",
    "        image = X\n",
    "        embedding = self.calculate_embedding(image)\n",
    "        projection = self.projection(embedding)\n",
    "        return embedding, projection\n",
    "\n",
    "    \n",
    "class ImageEmbeddingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        hparams = Namespace(**hparams) if isinstance(hparams, dict) else hparams\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = ImageEmbedding()\n",
    "        self.loss = SupConLoss()\n",
    "    \n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.epochs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data, labels,\n",
    "                                             debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=4,\n",
    "                          sampler=SubsetRandomSampler(list(range(hparams.train_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data, labels,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size=self.hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(hparams.train_size + 1, hparams.train_size + hparams.validation_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def step(self, batch, step_name = \"train\"):\n",
    "        (X, Y), labels = batch\n",
    "        embX, projectionX = self.forward(X)\n",
    "        embY, projectionY = self.forward(Y)\n",
    "        z_i = F.normalize(projectionX , dim=1)\n",
    "        z_j = F.normalize(projectionY, dim=1)\n",
    "        projX = torch.reshape(z_i,(z_i.shape[0],1,z_i.shape[1]))\n",
    "        projY = torch.reshape(z_j,(z_j.shape[0],1,z_j.shape[1]))\n",
    "        features = torch.cat([projX, projY], dim=1)\n",
    "        loss = self.loss(features=features,labels=labels)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "        self.log(\"loss\" if step_name == \"train\" else loss_key, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        #return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "                        #\"progress_bar\": {loss_key: loss}}\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        if len(outputs) == 0:\n",
    "            return {\"val_loss\": torch.tensor(0)}\n",
    "        else:\n",
    "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            return {\"val_loss\": loss, \"log\": {\"val_loss\": loss}}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RMSprop(self.model.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=15, verbose=True)\n",
    "        return {\n",
    "           'optimizer': optimizer,\n",
    "           'lr_scheduler': scheduler,\n",
    "           'monitor': 'val_loss'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, freeze_base, embeddings_model_path, hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_model = ImageEmbeddingModule.load_from_checkpoint(embeddings_model_path).model\n",
    "        \n",
    "        self.embeddings = base_model.embedding\n",
    "        \n",
    "        if freeze_base:\n",
    "            print(\"Freezing embeddings\")\n",
    "            for param in self.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Only linear projection on top of the embeddings should be enough\n",
    "        #self.classifier = nn.Linear(in_features=base_model.projection[0].in_features,out_features=n_classes)\n",
    "                      #out_features=n_classes if n_classes > 2 else 1)\n",
    "        self.classifier = nn.Sequential(nn.Linear(in_features=base_model.projection[0].in_features,out_features=hidden_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),                           \n",
    "                      nn.Linear(hidden_size, out_features=n_classes))\n",
    "    \n",
    "    def forward(self, X, *args):\n",
    "        emb = self.embeddings(X)\n",
    "        return self.classifier(emb)\n",
    "    \n",
    "class PretrainingDatasetWrapper_cls(Dataset):\n",
    "    def __init__(self, ds: Dataset, l: labels, target_size=(512,512), debug=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.labels = l\n",
    "        self.debug = debug\n",
    "        self.target_size = target_size\n",
    "        if debug:\n",
    "            print(\"DATASET IN DEBUG MODE\")\n",
    "        \n",
    "        # I will be using network pre-trained on ImageNet first, which uses this normalization.\n",
    "        # Remove this, if you're training from scratch or apply different transformations accordingly\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "            \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        this_image_raw_path = self.ds[idx]\n",
    "        label = self.labels[idx]\n",
    "        this_image_raw = Image.open(this_image_raw_path)\n",
    "        img = tvf.resize(this_image_raw, self.target_size)\n",
    "        if preprocess:\n",
    "            t1 = self.preprocess(img)\n",
    "        else:\n",
    "            t1 = transforms.ToTensor()(img)\n",
    "            \n",
    "        return t1, torch.tensor(label), this_image_raw_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "    \n",
    "class SimCLRClassifierModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        hparams = Namespace(**hparams) if isinstance(hparams, dict) else hparams\n",
    "        self.hparams = hparams\n",
    "        self.model = SimCLRClassifier(hparams.n_classes, hparams.freeze_base, \n",
    "                                      hparams.embeddings_path,\n",
    "                                      self.hparams.hidden_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = pl.metrics.classification.Accuracy()\n",
    "        \n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.epochs\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(data, labels,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size=self.hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SubsetRandomSampler(list(range(self.hparams.train_size))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(data, labels,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size=self.hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(self.hparams.train_size + 1, self.hparams.train_size + self.hparams.validation_size))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def test_dataloader(self,dataset):\n",
    "        trash = [1 for i in range(len(dataset))]\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(dataset, trash,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          #batch_size=self.hparams.batch_size, \n",
    "                          batch_size = 1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(len(dataset)))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def step(self, batch, step_name = \"train\"):\n",
    "        X, y, _ = batch\n",
    "        y_out = self.forward(X)\n",
    "        loss = self.loss(y_out, y)\n",
    "        accuracy = self.accuracy(y_out.argmax(dim=-1), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        accuracy_key = f\"{step_name}_acc\"\n",
    "        tensorboard_logs = {loss_key: loss, accuracy_key: accuracy}\n",
    "        self.log(\"loss\" if step_name == \"train\" else loss_key, loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\" if step_name == \"train\" else accuracy_key, accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "#                         \"progress_bar\": {loss_key: loss}}\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        if len(outputs) == 0:\n",
    "            return {\"val_loss\": torch.tensor(0)}\n",
    "        else:\n",
    "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "            return {\"val_loss\": loss, \"val_acc\":acc, \"log\": {\"val_loss\": loss,\"val_acc\":acc}}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RMSprop(self.model.parameters(), lr=self.hparams.lr)\n",
    "        schedulers = [\n",
    "            CosineAnnealingLR(optimizer, self.hparams.epochs)\n",
    "        ] if self.hparams.epochs > 1 else []\n",
    "        return [optimizer], schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Checkpoint directory /workstation/home/bijoy/data_from_b170007ec/Programs/Manoj/SQLEARNING/Checkpoints_cls exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b2-8bb594d6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1562a5944ca24251bfacc84d56a06628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=36804509.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "hparams_cls = Namespace(\n",
    "    lr=1e-3,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    n_classes=2,\n",
    "    freeze_base=False,\n",
    "    embeddings_path=\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Checkpoints/model-v1.ckpt\",\n",
    "    hidden_size=512,\n",
    "    train_size=8800,\n",
    "    validation_size=832\n",
    ")\n",
    "checkpoint_callback_cls = ModelCheckpoint(\n",
    "    dirpath=\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Checkpoints_cls\", \n",
    "    filename=\"model\", \n",
    "    monitor='val_loss',\n",
    "    verbose=True, \n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback_cls = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=10, verbose=1, mode='min')\n",
    "module_cls = SimCLRClassifierModule(hparams_cls)\n",
    "trainer_cls = pl.Trainer(gpus=1,\n",
    "                         max_epochs=hparams_cls.epochs,\n",
    "                         callbacks=[checkpoint_callback_cls,early_stop_callback_cls],\n",
    "                         replace_sampler_ddp = False,\n",
    "                         progress_bar_refresh_rate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n",
      "Freezing embeddings\n"
     ]
    }
   ],
   "source": [
    "module_cls = SimCLRClassifierModule.load_from_checkpoint(\n",
    "    checkpoint_path=\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Checkpoints_cls/model-v1.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in module_cls.model.embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in module_cls.model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*list(module_cls.model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_features_conv = nn.Sequential(*list(model[0].children())[0:2])\n",
    "#print(before_features_conv)\n",
    "block = nn.Sequential(*list(model[0].children())[2])\n",
    "#print(block)\n",
    "features_conv = nn.Sequential(*list(model[0].children())[3:-5])\n",
    "#print(features_conv)\n",
    "after_features_conv = nn.Sequential(*list(model[0].children())[-5:])\n",
    "#print(after_features_conv)\n",
    "z = nn.Flatten()\n",
    "classifier = model[1]\n",
    "#print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scclassifier, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG19 network\n",
    "        self.model = nn.Sequential(*list(module_cls.model.children()))\n",
    "        \n",
    "        self.before_features_conv = nn.Sequential(*list(self.model[0].children())[0:2])\n",
    "        self.block = nn.Sequential(*list(self.model[0].children())[2])\n",
    "        self.features_conv = nn.Sequential(*list(self.model[0].children())[3:-5])\n",
    "        self.after_features_conv = nn.Sequential(*list(self.model[0].children())[-5:])\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.model[1]\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.before_features_conv(x)\n",
    "        x = self.block(x)\n",
    "        x = self.features_conv(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.after_features_conv(x)\n",
    "        x = x.view((1, -1))\n",
    "        #x = nn.Flatten()(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        x = self.before_features_conv(x)\n",
    "        x = self.block(x)\n",
    "        x = self.features_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of additional data : 2780\n",
      "tensor([1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# files_ = os.listdir(\"/workstation/home/bijoy/data_from_b170007ec/Programs/Manoj/SQLEARNING/Imagenet/val\")\n",
    "# files_ = sorted(files_)\n",
    "# img_dirs_ = []\n",
    "# labels_ = []\n",
    "# for i in tqdm(files_[k:k+1]):\n",
    "#     path = \"/workstation/home/bijoy/data_from_b170007ec/Programs/Manoj/SQLEARNING/Imagenet/val/\" + str(i)\n",
    "#     img = Image.open(path)\n",
    "#     t2 = transforms.ToTensor()(img)\n",
    "#     if t2.shape[0] == 3:\n",
    "#         img_dirs_.append(path)\n",
    "#         labels_.append(1)\n",
    "# type1_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/good_files1.txt'\n",
    "# type2_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/files2good.txt'\n",
    "# type3_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/good_files3.txt'\n",
    "# test_data = []\n",
    "# test_labels = []\n",
    "# for class_,txt in enumerate([type1_txt,type2_txt,type3_txt]):\n",
    "#     for i,dir_ in enumerate(pickle.load(open(txt,'rb'))):\n",
    "#         dir_ = dir_.split('/')\n",
    "#         dir_[0] = '/workstation'\n",
    "#         dir_[1] = 'home'\n",
    "#         dir_.insert(2,'bijoy')\n",
    "#         dir_.insert(3,'data_from_b170007ec')\n",
    "#         dir_.insert(6,'train')\n",
    "#         if 'additional' in dir_[-3].split('_'):\n",
    "#             dir_.remove(dir_[-2])\n",
    "#         else:\n",
    "#             dir_.remove(dir_[-3])\n",
    "#         dir_ = '/'.join(dir_)\n",
    "#         test_data.append(dir_)\n",
    "#         test_labels.append(0)\n",
    "        \n",
    "additional_data = []\n",
    "additional_label = []\n",
    "bad_type1_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/bad_files1.txt'\n",
    "bad_type2_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/files2bad.txt'\n",
    "bad_type3_txt = '/workstation/home/bijoy/data_from_b170007ec/Programs/Cervix Cancer/bad_files3.txt'\n",
    "for class_,txt in enumerate([bad_type1_txt,bad_type2_txt,bad_type3_txt]):\n",
    "    for i,dir_ in enumerate(pickle.load(open(txt,'rb'))):\n",
    "        dir_ = dir_.split('/')\n",
    "        dir_[0] = '/workstation'\n",
    "        dir_[1] = 'home'\n",
    "        dir_.insert(2,'bijoy')\n",
    "        dir_.insert(3,'data_from_b170007ec')\n",
    "        dir_.insert(6,'train')\n",
    "        if 'additional' in dir_[-3].split('_'):\n",
    "            dir_.remove(dir_[-2])\n",
    "        else:\n",
    "            dir_.remove(dir_[-3])\n",
    "        dir_ = '/'.join(dir_)\n",
    "        additional_data.append(dir_)\n",
    "        additional_label.append(1)\n",
    "print(\"Length of additional data :\",len(additional_data))\n",
    "s = Scclassifier()\n",
    "s.eval()\n",
    "dataloader = DataLoader(PretrainingDatasetWrapper_cls(additional_data[k:k+1], additional_label[k:k+1],\n",
    "                                            debug=getattr(hparams_cls, \"debug\", False)),\n",
    "                          batch_size = 1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(len(additional_data)))),\n",
    "                         drop_last=False)\n",
    "img, label, path = next(iter(dataloader))\n",
    "pred = s(img)\n",
    "\n",
    "pred[:,0].backward(retain_graph = True)\n",
    "print(pred.argmax(dim=1))\n",
    "# pull the gradients out of the model\n",
    "gradients = s.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = s.get_activations(img).detach()\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(1408):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(path[0])\n",
    "img = cv2.resize(img,(512,512))\n",
    "heatmap = cv2.resize(np.float32(heatmap), (img.shape[1], img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.6 + img\n",
    "im_v = np.concatenate((img, superimposed_img), axis=0)\n",
    "cv2.imwrite('cervix{}.jpg'.format(k), im_v)\n",
    "k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
