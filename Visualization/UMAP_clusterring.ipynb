{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data : 14226\n",
      "Length of labels : 14226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import albumentations as A\n",
    "import torch\n",
    "torch.__version__\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.multiprocessing import cpu_count\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Output_csv/Model_binary_v3.csv\", usecols=range(1,5))\n",
    "predicted_noise = df[df[\"Predicted_label\"]==1]\n",
    "cervix = df[(df[\"Predicted_label\"] == 0)]\n",
    "cervix = cervix[cervix[\"Data_type\"] == \"Train\"]\n",
    "\n",
    "test_ext = pd.read_csv(\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/TESTING_ON_5DATASETS/testing_data.csv\")\n",
    "\n",
    "imagenetdata = test_ext[test_ext[\"imagenet\"] == 1]\n",
    "blooddata = test_ext[test_ext[\"blood\"] == 1]\n",
    "eyedata = test_ext[test_ext[\"eye\"] == 1]\n",
    "skindata = test_ext[test_ext[\"skin\"] == 1]\n",
    "surgerydata = test_ext[test_ext[\"surgery\"] == 1]\n",
    "\n",
    "img_dirs = []\n",
    "labels_temp = []\n",
    "for a in list(predicted_noise[\"Path\"]):\n",
    "    img_dirs.append(a)\n",
    "    labels_temp.append(0)\n",
    "for b in list(cervix[\"Path\"]):\n",
    "    img_dirs.append(b)\n",
    "    labels_temp.append(1)\n",
    "for j in list(imagenetdata[\"Path\"]):\n",
    "    img_dirs.append(j)\n",
    "    labels_temp.append(2)\n",
    "for k in list(blooddata[\"Path\"]):\n",
    "    img_dirs.append(k)\n",
    "    labels_temp.append(3)\n",
    "for l in list(eyedata[\"Path\"]):\n",
    "    img_dirs.append(l)\n",
    "    labels_temp.append(4)\n",
    "for m in list(skindata[\"Path\"]):\n",
    "    img_dirs.append(m)\n",
    "    labels_temp.append(5)\n",
    "for n in list(surgerydata[\"Path\"]):\n",
    "    img_dirs.append(n)\n",
    "    labels_temp.append(6)\n",
    "\n",
    "data = img_dirs\n",
    "labels = labels_temp\n",
    "# data = data_temp\n",
    "# labels = labels_temp\n",
    "print(\"Length of data :\",len(data))\n",
    "print(\"Length of labels :\",len(labels))\n",
    "\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "       \n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "     \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def random_rotate(image):\n",
    "    if random.random() > 0.5:\n",
    "        return tvf.rotate(image, angle=random.choice((0, 90, 180, 270)))\n",
    "    return image\n",
    "\n",
    "class ResizedRotation():\n",
    "    def __init__(self, angle, output_size=(512,512)):\n",
    "        self.angle = angle\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def angle_to_rad(self, ang): return np.pi * ang / 180.0\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        new_h = int(np.abs(w * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(h * np.sin(self.angle_to_rad(self.angle))))\n",
    "        new_w = int(np.abs(h * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(w * np.sin(self.angle_to_rad(self.angle))))\n",
    "        img = tvf.resize(image, (new_w, new_h))\n",
    "        img = tvf.rotate(img, self.angle)\n",
    "        img = tvf.center_crop(img, self.output_size)\n",
    "        return img\n",
    "    \n",
    "class WrapWithRandomParams():\n",
    "    def __init__(self, constructor, ranges):\n",
    "        self.constructor = constructor\n",
    "        self.ranges = ranges\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        randoms = [float(np.random.uniform(low, high)) for _, (low, high) in zip(range(len(self.ranges)), self.ranges)]\n",
    "        return self.constructor(*randoms)(image)\n",
    "    \n",
    "\n",
    "class PretrainingDatasetWrapper(Dataset):\n",
    "    def __init__(self, ds: Dataset, l: labels, target_size=(512,512), debug=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.labels = labels\n",
    "        self.debug = debug\n",
    "        self.target_size = target_size\n",
    "        if debug:\n",
    "            print(\"DATASET IN DEBUG MODE\")\n",
    "        \n",
    "        # I will be using network pre-trained on ImageNet first, which uses this normalization.\n",
    "        # Remove this, if you're training from scratch or apply different transformations accordingly\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        random_resized_rotation = WrapWithRandomParams(lambda angle: ResizedRotation(angle, target_size), [(0.0, 360.0)])\n",
    "        self.randomize = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(target_size, scale=(330/512, 330/512), ratio=(1.0, 1.0)),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Lambda(random_rotate)\n",
    "            ]),\n",
    "            transforms.RandomApply([\n",
    "                random_resized_rotation\n",
    "            ], p=0.6),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.6, saturation=0.5, hue=0.2)\n",
    "            ], p=0.8)\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        this_image_raw_path = self.ds[idx]\n",
    "        label = self.labels[idx]\n",
    "        this_image_raw = Image.open(this_image_raw_path)\n",
    "        if self.debug:\n",
    "            random.seed(idx)\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            random.seed(idx + 1)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        else:\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        \n",
    "        if preprocess:\n",
    "            t1 = self.preprocess(t1)\n",
    "            t2 = self.preprocess(t2)\n",
    "        else:\n",
    "            t1 = transforms.ToTensor()(t1)\n",
    "            t2 = transforms.ToTensor()(t2)\n",
    "            \n",
    "        return (t1, t2), torch.tensor(label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "    \n",
    "    \n",
    "    \n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class ImageEmbedding(nn.Module):       \n",
    "    class Identity(nn.Module):\n",
    "        def __init__(self): super().__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "    \n",
    "        \n",
    "    def __init__(self, embedding_size=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        #base_model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "        base_model = EfficientNet.from_name(\"efficientnet-b2\")\n",
    "        internal_embedding_size = base_model._fc.in_features\n",
    "        base_model._fc = ImageEmbedding.Identity()\n",
    "        \n",
    "        self.embedding = base_model\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=internal_embedding_size, out_features=embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embedding_size, out_features=embedding_size)\n",
    "        )\n",
    "\n",
    "    def calculate_embedding(self, image):\n",
    "        return self.embedding(image)\n",
    "\n",
    "    def forward(self, X):\n",
    "        image = X\n",
    "        embedding = self.calculate_embedding(image)\n",
    "        projection = self.projection(embedding)\n",
    "        return embedding, projection\n",
    "\n",
    "hparams = Namespace(\n",
    "    lr=0.0001,\n",
    "    epochs=1,\n",
    "    batch_size=64,\n",
    "    train_size=37184,\n",
    "    validation_size=4150\n",
    ")\n",
    "    \n",
    "class ImageEmbeddingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams = hparams):\n",
    "        hparams = Namespace(**hparams) if isinstance(hparams, dict) else hparams\n",
    "        super().__init__()\n",
    "        #self.hparams = hparams\n",
    "        self.model = ImageEmbedding()\n",
    "        self.loss = SupConLoss()\n",
    "    \n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // hparams.epochs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data,labels,\n",
    "                                             debug=getattr(hparams, \"debug\", False)),\n",
    "                          batch_size=hparams.batch_size, \n",
    "                          num_workers=4,#cpu_count(),\n",
    "                          sampler=SubsetRandomSampler(list(range(hparams.train_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data,labels,\n",
    "                                            debug=getattr(hparams, \"debug\", False)),\n",
    "                          batch_size=hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,#cpu_count(),\n",
    "                          sampler=SequentialSampler(list(range(hparams.train_size + 1, hparams.train_size + hparams.validation_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def step(self, batch, step_name = \"train\"):\n",
    "        (X, Y), labels = batch\n",
    "        embX, projectionX = self.forward(X)\n",
    "        embY, projectionY = self.forward(Y)\n",
    "        z_i = F.normalize(projectionX , dim=1)\n",
    "        z_j = F.normalize(projectionY, dim=1)\n",
    "        projX = torch.reshape(z_i,(z_i.shape[0],1,z_i.shape[1]))\n",
    "        projY = torch.reshape(z_j,(z_j.shape[0],1,z_j.shape[1]))\n",
    "        features = torch.cat([projX, projY], dim=1)\n",
    "        loss = self.loss(features=features,labels=labels)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "        self.log(\"loss\" if step_name == \"train\" else loss_key, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        #return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "                        #\"progress_bar\": {loss_key: loss}}\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        if len(outputs) == 0:\n",
    "            return {\"val_loss\": torch.tensor(0)}\n",
    "        else:\n",
    "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            return {\"val_loss\": loss, \"log\": {\"val_loss\": loss}}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RMSprop(self.model.parameters(), lr=hparams.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=15, verbose=True)\n",
    "        return {\n",
    "           'optimizer': optimizer,\n",
    "           'lr_scheduler': scheduler,\n",
    "           'monitor': 'val_loss'\n",
    "       }\n",
    "hparams = Namespace(\n",
    "    lr=0.0001,\n",
    "    epochs=500,\n",
    "    batch_size=22,\n",
    "    train_size=33396,\n",
    "    validation_size=3708\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageEmbeddingModule.load_from_checkpoint(\n",
    "    checkpoint_path=\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Checkpoints/model-v3.ckpt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from matplotlib import cm\n",
    "from skimage import img_as_ubyte\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sklearn\n",
    "import llvmlite\n",
    "import umap.umap_ as umap\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def draw_umap(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', title=''):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    classes = [0,1,2,3,4,5]\n",
    "    colors=ListedColormap([\"green\", \"orange\",  \"blue\", \"#550011\", \"purple\", \"red\"])\n",
    "    u = fit.fit_transform(latent_space);\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    try:\n",
    "        if n_components == 1:\n",
    "            ax = fig.add_subplot(111)\n",
    "            scatter = ax.scatter(u[:,0], range(len(u)), c = labels,cmap=colors)\n",
    "        if n_components == 2:\n",
    "            ax = fig.add_subplot(111)\n",
    "            scatter = ax.scatter(u[:,0], u[:,1], s=1, c = labels,cmap=colors)\n",
    "        if n_components == 3:\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            scatter = ax.scatter(u[:,0], u[:,1], u[:,2], s=1, c = labels,cmap=colors)\n",
    "        plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.savefig(\"Umap_plot_SClearning_test_{}\".format(n_neighbors))\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "latent_space = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for path in tqdm(data[:2]):\n",
    "    projection = np.zeros((1, 1024))\n",
    "    this_image_raw = Image.open(path)\n",
    "    img = tvf.resize(this_image_raw, (512,512))\n",
    "    t1 = preprocess(img)\n",
    "    t2 = t1.unsqueeze(0)\n",
    "    embeddings, projections = model(t2)\n",
    "    projection[0,:] = projections.cpu().detach().numpy()\n",
    "    latent_space.append(projection)\n",
    "latent_space = np.array(latent_space)\n",
    "print(latent_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 1024)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "image_features_arr = np.rollaxis(latent_space,1,0)\n",
    "print(image_features_arr.shape)\n",
    "image_features_arr = image_features_arr[0,:,:]\n",
    "\n",
    "np.savetxt('feature_vectors_samples.txt',image_features_arr)\n",
    "#feature_vectors = np.loadtxt('feature_vectors.txt')\n",
    "pickle.dump(image_features_arr, open('feature_vectors_samples.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = draw_umap(n_neighbors=10000, min_dist = 0.1, n_components=2, metric='euclidean', title='UMAP-TEST DATASET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Noise</th>\n",
       "      <th>Data_type</th>\n",
       "      <th>Predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/workstation/home/bijoy/data_from_b170007ec/Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/workstation/home/bijoy/data_from_b170007ec/Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/workstation/home/bijoy/data_from_b170007ec/Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/workstation/home/bijoy/data_from_b170007ec/Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/workstation/home/bijoy/data_from_b170007ec/Da...</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path  Noise Data_type  \\\n",
       "0  /workstation/home/bijoy/data_from_b170007ec/Da...      0     Train   \n",
       "1  /workstation/home/bijoy/data_from_b170007ec/Da...      0     Train   \n",
       "2  /workstation/home/bijoy/data_from_b170007ec/Da...      0     Train   \n",
       "3  /workstation/home/bijoy/data_from_b170007ec/Da...      0     Train   \n",
       "4  /workstation/home/bijoy/data_from_b170007ec/Da...      0     Train   \n",
       "\n",
       "   Predicted_label  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/workstation/home/bijoy/data_from_b170007ec/Programs/Bhanu/SCLEARNING/Output_csv/Model_binary_v3.csv\", usecols=range(1,5))\n",
    "predicted_noise = df[df[\"Predicted_label\"]==1]\n",
    "cervix = df[(df[\"Predicted_label\"] == 0)]\n",
    "cervix = cervix[cervix[\"Data_type\"] == \"Train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
