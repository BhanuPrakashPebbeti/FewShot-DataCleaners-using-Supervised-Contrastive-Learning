{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pip albumentations==0.4.5 pytorch-lightning\n",
    "!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from random import shuffle\n",
    "import albumentations as A\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from PIL import Image\n",
    "from torch.multiprocessing import cpu_count\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from argparse import Namespace\n",
    "import xml.etree.ElementTree as ET\n",
    "    \n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "       \n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "     \n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def random_rotate(image):\n",
    "    if random.random() > 0.5:\n",
    "        return tvf.rotate(image, angle=random.choice((0, 90, 180, 270)))\n",
    "    return image\n",
    "\n",
    "class ResizedRotation():\n",
    "    def __init__(self, angle, output_size=(512,512)):\n",
    "        self.angle = angle\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def angle_to_rad(self, ang): return np.pi * ang / 180.0\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        new_h = int(np.abs(w * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(h * np.sin(self.angle_to_rad(self.angle))))\n",
    "        new_w = int(np.abs(h * np.sin(self.angle_to_rad(90 - self.angle))) + np.abs(w * np.sin(self.angle_to_rad(self.angle))))\n",
    "        img = tvf.resize(image, (new_w, new_h))\n",
    "        img = tvf.rotate(img, self.angle)\n",
    "        img = tvf.center_crop(img, self.output_size)\n",
    "        return img\n",
    "    \n",
    "class WrapWithRandomParams():\n",
    "    def __init__(self, constructor, ranges):\n",
    "        self.constructor = constructor\n",
    "        self.ranges = ranges\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        randoms = [float(np.random.uniform(low, high)) for _, (low, high) in zip(range(len(self.ranges)), self.ranges)]\n",
    "        return self.constructor(*randoms)(image)\n",
    "    \n",
    "class PretrainingDatasetWrapper(Dataset):\n",
    "    def __init__(self, ds, l, target_size=(512,512), debug=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.labels = l\n",
    "        self.debug = debug\n",
    "        self.target_size = target_size\n",
    "        if debug:\n",
    "            print(\"DATASET IN DEBUG MODE\")\n",
    "        \n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        random_resized_rotation = WrapWithRandomParams(lambda angle: ResizedRotation(angle, target_size), [(0.0, 360.0)])\n",
    "        self.randomize = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(target_size, scale=(340/512, 340/512), ratio=(1.0, 1.0)),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Lambda(random_rotate)\n",
    "            ]),\n",
    "            transforms.RandomApply([\n",
    "                random_resized_rotation\n",
    "            ], p=0.6),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.6, saturation=0.6, hue=0.3)\n",
    "            ], p=0.8)\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        this_image_raw_path = self.ds[idx]\n",
    "        label = self.labels[idx]\n",
    "        this_image_raw = Image.open(this_image_raw_path)\n",
    "        if self.debug:\n",
    "            random.seed(idx)\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            random.seed(idx + 1)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        else:\n",
    "            t1 = self.randomize(this_image_raw)\n",
    "            t2 = self.randomize(this_image_raw)\n",
    "        \n",
    "        if preprocess:\n",
    "            t1 = self.preprocess(t1)\n",
    "            t2 = self.preprocess(t2)\n",
    "        else:\n",
    "            t1 = transforms.ToTensor()(t1)\n",
    "            t2 = transforms.ToTensor()(t2)\n",
    "            \n",
    "        return (t1, t2), torch.tensor(label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "    \n",
    "class ImageEmbedding(nn.Module):       \n",
    "    class Identity(nn.Module):\n",
    "        def __init__(self): super().__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "    \n",
    "        \n",
    "    def __init__(self, embedding_size=1024):\n",
    "        super().__init__()\n",
    "        base_model = EfficientNet.from_name(\"efficientnet-b2\")\n",
    "        internal_embedding_size = base_model._fc.in_features\n",
    "        base_model._fc = ImageEmbedding.Identity()\n",
    "        \n",
    "        self.embedding = base_model\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features=internal_embedding_size, out_features=embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=embedding_size, out_features=embedding_size)\n",
    "        )\n",
    "\n",
    "    def calculate_embedding(self, image):\n",
    "        return self.embedding(image)\n",
    "\n",
    "    def forward(self, X):\n",
    "        image = X\n",
    "        embedding = self.calculate_embedding(image)\n",
    "        projection = self.projection(embedding)\n",
    "        return embedding, projection\n",
    "    \n",
    "hparams = Namespace(\n",
    "    lr=0.001,\n",
    "    epochs=200,\n",
    "    batch_size=51,\n",
    "    train_size=63580,\n",
    "    validation_size=7030\n",
    ")\n",
    "    \n",
    "class ImageEmbeddingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams = hparams):\n",
    "        super().__init__()\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = ImageEmbedding()\n",
    "        self.loss = SupConLoss()\n",
    "    \n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // hparams.epochs\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data,labels,\n",
    "                                             debug=getattr(hparams, \"debug\", False)),\n",
    "                          batch_size=hparams.batch_size, \n",
    "                          num_workers=4,#cpu_count(),\n",
    "                          sampler=SubsetRandomSampler(list(range(hparams.train_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper(data,labels,\n",
    "                                            debug=getattr(hparams, \"debug\", False)),\n",
    "                          batch_size=hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,#cpu_count(),\n",
    "                          sampler=SequentialSampler(list(range(hparams.train_size + 1, hparams.train_size + hparams.validation_size))),\n",
    "                         drop_last=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def step(self, batch, step_name = \"train\"):\n",
    "        (X, Y), labels = batch\n",
    "        embX, projectionX = self.forward(X)\n",
    "        embY, projectionY = self.forward(Y)\n",
    "        z_i = F.normalize(projectionX , dim=1)\n",
    "        z_j = F.normalize(projectionY, dim=1)\n",
    "        projX = torch.reshape(z_i,(z_i.shape[0],1,z_i.shape[1]))\n",
    "        projY = torch.reshape(z_j,(z_j.shape[0],1,z_j.shape[1]))\n",
    "        features = torch.cat([projX, projY], dim=1)\n",
    "        loss = self.loss(features=features,labels=labels)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "        self.log(\"loss\" if step_name == \"train\" else loss_key, loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        if len(outputs) == 0:\n",
    "            return {\"val_loss\": torch.tensor(0)}\n",
    "        else:\n",
    "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            return {\"val_loss\": loss, \"log\": {\"val_loss\": loss}}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RMSprop(self.model.parameters(), lr=hparams.lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=15, verbose=True)\n",
    "        return {\n",
    "           'optimizer': optimizer,\n",
    "           'lr_scheduler': scheduler,\n",
    "           'monitor': 'val_loss'\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, freeze_base, embeddings_model_path, hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        base_model = ImageEmbeddingModule.load_from_checkpoint(embeddings_model_path).model\n",
    "        \n",
    "        self.embeddings = base_model.embedding\n",
    "        \n",
    "        if freeze_base:\n",
    "            print(\"Freezing embeddings\")\n",
    "            for param in self.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Only linear projection on top of the embeddings should be enough\n",
    "        self.classifier = nn.Sequential(nn.Linear(in_features=base_model.projection[0].in_features,out_features=hidden_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.25),                           \n",
    "                      nn.Linear(hidden_size, out_features=n_classes))\n",
    "    \n",
    "    def forward(self, X, *args):\n",
    "        emb = self.embeddings(X)\n",
    "        return self.classifier(emb)\n",
    "    \n",
    "class PretrainingDatasetWrapper_cls(Dataset):\n",
    "    def __init__(self, ds, l, target_size=(512,512), debug=False):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.labels = l\n",
    "        self.debug = debug\n",
    "        self.target_size = target_size\n",
    "        if debug:\n",
    "            print(\"DATASET IN DEBUG MODE\")\n",
    "        \n",
    "        # I will be using network pre-trained on ImageNet first, which uses this normalization.\n",
    "        # Remove this, if you're training from scratch or apply different transformations accordingly\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        random_resized_rotation = WrapWithRandomParams(lambda angle: ResizedRotation(angle, target_size), [(0.0, 360.0)])\n",
    "        self.randomize = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(target_size, scale=(340/512, 340/512), ratio=(1.0, 1.0)),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Lambda(random_rotate)\n",
    "            ]),\n",
    "            transforms.RandomApply([\n",
    "                random_resized_rotation\n",
    "            ], p=0.6)\n",
    "        ])\n",
    "            \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        this_image_raw_path = self.ds[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(this_image_raw_path)\n",
    "        img = tvf.resize(img, self.target_size)\n",
    "#         img = self.randomize(img)\n",
    "        if preprocess:\n",
    "            t1 = self.preprocess(img)\n",
    "        else:\n",
    "            t1 = transforms.ToTensor()(img)\n",
    "            \n",
    "        return t1, torch.tensor(label).to(torch.float32), this_image_raw_path\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "\n",
    "class SimCLRClassifierModule(pl.LightningModule):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__()\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.model = SimCLRClassifier(self.hparams.n_classes, self.hparams.freeze_base, \n",
    "                                      self.hparams.embeddings_path,\n",
    "                                      self.hparams.hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        \n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.epochs\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(data, labels,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size=self.hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SubsetRandomSampler(list(range(self.hparams.train_size))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(data, labels,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size = self.hparams.batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(self.hparams.train_size + 1, self.hparams.train_size + self.hparams.validation_size))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def test_dataloader(self,dataset):\n",
    "        trash = [0 for i in range(len(dataset))]\n",
    "        return DataLoader(PretrainingDatasetWrapper_cls(dataset, trash,\n",
    "                                            debug=getattr(self.hparams, \"debug\", False)),\n",
    "                          batch_size = 1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(len(dataset)))),\n",
    "                         drop_last=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = self.model(X)\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def step(self, batch, step_name = \"train\"):\n",
    "        X, y, _ = batch\n",
    "        y_out = self.forward(X)\n",
    "        loss = self.loss(y_out, y.unsqueeze(1))\n",
    "        y_pred_tag = torch.round(y_out)\n",
    "        y = y.to(torch.int32)\n",
    "        accuracy = self.accuracy(y_pred_tag.squeeze(1), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        accuracy_key = f\"{step_name}_acc\"\n",
    "        tensorboard_logs = {loss_key: loss, accuracy_key: accuracy}\n",
    "        self.log(\"loss\" if step_name == \"train\" else loss_key, loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"acc\" if step_name == \"train\" else accuracy_key, accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        if len(outputs) == 0:\n",
    "            return {\"val_loss\": torch.tensor(0)}\n",
    "        else:\n",
    "            loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "            acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "            return {\"val_loss\": loss, \"val_acc\":acc, \"log\": {\"val_loss\": loss,\"val_acc\":acc}}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RMSprop(self.model.parameters(), lr=self.hparams.lr)\n",
    "        schedulers = [\n",
    "            CosineAnnealingLR(optimizer, self.hparams.epochs)\n",
    "        ] if self.hparams.epochs > 1 else []\n",
    "        return [optimizer], schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_cls = Namespace(\n",
    "    lr=1e-3,\n",
    "    epochs=10,\n",
    "    batch_size=20,\n",
    "    n_classes=1,\n",
    "    freeze_base=True,\n",
    "    embeddings_path=\"/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/Checkpoints/model.ckpt\",\n",
    "    hidden_size=512,\n",
    "    train_size=1250,\n",
    "    validation_size=22000\n",
    ")\n",
    "checkpoint_callback_cls = ModelCheckpoint(\n",
    "    dirpath=\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/Checkpoints_cls\", \n",
    "    filename=\"model\", \n",
    "    monitor='val_loss',\n",
    "    verbose=True, \n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "module_cls = SimCLRClassifierModule(hparams_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/Checkpoints_cls/Cervix/model_100_aug.ckpt\")\n",
    "module_cls.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in module_cls.model.embeddings.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in module_cls.model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*list(module_cls.model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scclassifier, self).__init__()\n",
    "        self.model = nn.Sequential(*list(module_cls.model.children()))\n",
    "        \n",
    "        self.before_features_conv = nn.Sequential(*list(self.model[0].children())[0:2])\n",
    "        self.block = nn.Sequential(*list(self.model[0].children())[2])\n",
    "        self.features_conv = nn.Sequential(*list(self.model[0].children())[3:-5])\n",
    "        self.after_features_conv = nn.Sequential(*list(self.model[0].children())[-5:])\n",
    "        \n",
    "        self.classifier = self.model[1]\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.before_features_conv(x)\n",
    "        x = self.block(x)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        x = self.features_conv(x)\n",
    "        x = self.after_features_conv(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        x = self.before_features_conv(x)\n",
    "        x = self.block(x)\n",
    "        x = self.features_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0\n",
    "\n",
    "df = pd.read_csv(\"/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/Classifier/Cervix/100_aug/Dataset_100_aug.csv\")\n",
    "predicted_noise = df[df[\"Predicted_label\"]==1]\n",
    "cervix = df[(df[\"Predicted_label\"] == 0)]\n",
    "cervix = cervix[cervix[\"Data_type\"] == \"Train\"]\n",
    "\n",
    "test_ext = pd.read_csv(\"/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/Dataset/Dataset1.csv\")\n",
    "cervix_t = test_ext[test_ext[\"id\"] == 1]\n",
    "blooddata = test_ext[test_ext[\"id\"] == 0]\n",
    "eyedata = test_ext[test_ext[\"id\"] == 2]\n",
    "skindata = test_ext[test_ext[\"id\"] == 3]\n",
    "surgerydata = test_ext[test_ext[\"id\"] == 4]\n",
    "test_ext.drop(cervix_t.index, axis=0,inplace=True)\n",
    "test_ext.drop(blooddata.index, axis=0,inplace=True)\n",
    "test_ext.drop(eyedata.index, axis=0,inplace=True)\n",
    "test_ext.drop(skindata.index, axis=0,inplace=True)\n",
    "test_ext.drop(surgerydata.index, axis=0,inplace=True)\n",
    "imagenetdata = test_ext.sample(n=100)\n",
    "\n",
    "img_dirs = []\n",
    "labels = []\n",
    "\n",
    "# for a in list(predicted_noise[\"Path\"]):\n",
    "#     a = a.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(a)\n",
    "#     labels.append(0)\n",
    "# for b in list(cervix[\"Path\"])[:500]:\n",
    "#     b = b.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(b)\n",
    "#     labels.append(1)\n",
    "# for j in list(imagenetdata[\"path\"])[:100]:\n",
    "#     j = j.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(j)\n",
    "#     labels.append(2)\n",
    "# for k in list(blooddata[\"path\"])[:100]:\n",
    "#     k = k.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(k)\n",
    "#     labels.append(3)\n",
    "# for l in list(eyedata[\"path\"])[:100]:\n",
    "#     l = l.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(l)\n",
    "#     labels.append(4)\n",
    "# for m in list(skindata[\"path\"])[:500]:\n",
    "#     m = m.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "#     img_dirs.append(m)\n",
    "#     labels.append(5)\n",
    "for n in list(surgerydata[\"path\"])[:100]:\n",
    "    n = n.replace(\"/workstation/raid/home/p170059cs/bijoy_backup/data_from_b170007ec\",\"/workstation/raid/home/p170059cs/data_from_b170007ec\")\n",
    "    img_dirs.append(n)\n",
    "    labels.append(6)\n",
    "\n",
    "additional_data = img_dirs\n",
    "additional_label = np.asarray(labels)\n",
    "print(\"Length of data :\",len(additional_data))\n",
    "print(\"Length of labels :\",len(additional_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scclassifier()\n",
    "s.eval()\n",
    "dataloader = DataLoader(PretrainingDatasetWrapper_cls(additional_data[z:z+100], additional_label[z:z+100],\n",
    "                                            debug=getattr(hparams_cls, \"debug\", False)),\n",
    "                          batch_size = 1,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          sampler=SequentialSampler(list(range(len(additional_data)))),\n",
    "                         drop_last=False)\n",
    "\n",
    "for img, label, path in dataloader:\n",
    "    pred = s(img)\n",
    "    pred[:,0].backward(retain_graph = True)\n",
    "    gradients = s.get_activations_gradient()\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    activations = s.get_activations(img).detach()\n",
    "    for i in range(352):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= torch.max(heatmap)\n",
    "    img = cv2.imread(path[0])\n",
    "#     img = cv2.resize(img,(512,512))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.resize(np.float32(heatmap), (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].imshow(superimposed_img/255.0)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    fig.savefig(\"./gradcams/Surgery/Gradcam_surgery-{}.png\".format(z))\n",
    "    plt.show()\n",
    "    z+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/gradcams/Cervix\")\n",
    "print(sorted(files)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/gradcams.zip'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('gradcams', format='zip', root_dir='/workstation/raid/home/p170059cs/data_from_b170007ec/Programs/Bhanu/CERVICAL2.0/gradcams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
